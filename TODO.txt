CHECK IF I CAN REMOVE INPUTTING THE INPUT POSE OF THE DESIRED APPEARANCE
IMAGE INTO THE ENCODER, would make things simpler

Refactor so OpenPose works entirely in pixel space and not in float space

Laplace of output

Maybe automatically add things like date to exp name. (Basically have better experiment names for searching etc)
(Also look at how sampling is done in a standard VAE)
Think about best croppings to feed into appearance encoder
THe wierd laplacian thing
Batch Norm
Bigger and cleaner data
Refactor pose detector code
Do they do fine tuning of VGG? Paper says they do something like it...
Inception Score monitoring
dropout
augmentation

Possibly formulate as a metalearning problem to encorporate multiple images of the same person

Better hands and face using: https://github.com/CMU-Perceptual-Computing-Lab/openpose

Maybe remove the import Fashion_Killer.hyperparams as hp dependency from so much of the code.
I think it will make testing certain bits of the pipeline in isolation harder in the future

Image generator requires passing in coordinates of joints.
Change this once OpenPose is incorporated correctly.

Seperate learning rate for VGG part.
Remove unneeded layers from VGG to free up space.
Initialise properly (i.e with low values)
Sigmoid or softmax on heatmap output (think there is something in
paper about this...)
Remove /tmp saving when done with diagnosis (maybe just comment out)
Maybe pass along the non sigmoided thing down net.
AFTER I have it working, see if I can get away with a shallower net.
Add unittests to training of pose_detector and whole of fashion killer.
